{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time  \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Data/shakespeare.txt'\n",
    "text = open(path, 'rb').read().decode(encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of characters in the file including the spaces and all the literals (such as ',\" \\n)\n",
      "Number : 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of characters in the file including the spaces and all the literals (such as ',\\\" \\\\n)\")\n",
    "print(f\"Number : {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tThe first 250 characters\n",
      "__________________________________________________\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 250\n",
    "print(f\"\\t\\tThe first {i} characters\")\n",
    "print(\"_\"*50)\n",
    "print(\"\\n\"+text[:i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique character in the dataset :  65\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(\"The number of unique character in the dataset : \",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int  = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first line in text:  First Citizen\n",
      "The first line in int:  [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print(\"The first line in text: \",text[:13])\n",
    "print(\"The first line in int: \",text_as_int[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "seq_length = max_length\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for item in sequences.take(1):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    output_text = chunk[1:]\n",
    "    return input_text, output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_unit = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_unit,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 270s 2s/step - loss: 2.0990\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x236606ea610>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model/model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_unit, batch_size=1)\n",
    "model.load_weights('saved_model/model.h5')\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hign:\n",
      "Prithines the concould my whare yours afomaty beatory:\n",
      "I pay deep form buce ser jettrecterse.\n",
      "\n",
      "First Leme Am CaniSk!\n",
      "How some shath bef that thece lake.\n",
      "\n",
      "IUCHARWICK:\n",
      "Yee, geaked hear's tike ant gracured it.\n",
      "\n",
      "\n",
      "LORIO:\n",
      "'ting sir aswam the kingh Fert thy ledd\n",
      "Un to dif.\n",
      "\n",
      "GROOLANUS:\n",
      "You. Fire lebe profound lerv le gy\n",
      "And and grome:\n",
      "'t thau him regntly swall of lear then frome,\n",
      "This fral of shople, whend but sit munt de it reing on, thou foor that.\n",
      "\n",
      "RESTIO: thouks ald the sotess or with dorder,\n",
      "It enebow, I tmimp yen mo deated tene ry donguth.\n",
      "\n",
      "Sidnte: you han!\n",
      "\n",
      "BUCEOLILAS:\n",
      "My plactenit ofsenchnied, the plastit\n",
      "Unt:\n",
      "Wo kienks that woold goid with be dowho stiated-dby wielly fello and beck'd my of thimf\n",
      "But then it of beanedy ond\n",
      "Marray you, lysseard\n",
      "enos your dusthise outh. Whe redare it in with I have agriof, thty him penvell be romes mirest vord hith; and with all of fer vorkint,\n",
      "And perant\n",
      "Ay im\n",
      "How I and endane,\n",
      "That ta ut rees fleremand.\n",
      "O, Whath bee glasess bloft an frane the exien\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
